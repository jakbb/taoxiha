package com.taoxiha.service.crawl.impl;import java.util.ArrayList;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.util.CollectionUtils;import com.taoxiha.base.dao.CrawlConfDao;import com.taoxiha.base.dao.DataSourcePoolDao;import com.taoxiha.base.dao.IpProxyPoolDao;import com.taoxiha.base.dao.MetadataDao;import com.taoxiha.base.dao.ResourceConfDao;import com.taoxiha.base.model.CrawlConf;import com.taoxiha.base.model.DataSourcePool;import com.taoxiha.base.model.IpProxyPool;import com.taoxiha.base.model.Metadata;import com.taoxiha.base.model.ResourceConf;import com.taoxiha.common.crawl.crawler.CrawlConfig;import com.taoxiha.common.crawl.crawler.CrawlController;import com.taoxiha.common.crawl.fetcher.PageFetcher;import com.taoxiha.common.crawl.robotstxt.RobotstxtConfig;import com.taoxiha.common.crawl.robotstxt.RobotstxtServer;import com.taoxiha.common.utils.StringUtils;import com.taoxiha.common.utils.log.LogUtils;import com.taoxiha.service.crawl.ICrawlService;import com.taoxiha.service.crawl.entity.CrawlInfoConfig;import com.taoxiha.service.crawl.entity.CrawlParams;import com.taoxiha.service.crawl.entity.CrawlResult;import com.taoxiha.service.crawl.entity.ReqParams;import com.taoxiha.service.crawl.entity.UrlRegFilters;import com.taoxiha.service.exception.ServiceException;import com.taoxiha.service.exception.ServiceException.Error;@Servicepublic class CrawlServiceImpl implements ICrawlService {		@Autowired	private ResourceConfDao resourceConfDao;	@Autowired	private DataSourcePoolDao dataSourceDao;	@Autowired	private CrawlConfDao crawlConfDao;	@Autowired	private IpProxyPoolDao ipProxyPoolDao;	@Autowired	private MetadataDao metadataDao;	@Override	public void crawlByResourceConfNum(String resource_conf_num,String taskNum)			throws ServiceException {		ResourceConf conf= resourceConfDao.getByResourceConfNum(resource_conf_num);				if(null==conf || null==conf.getResourceConfNum()){			LogUtils.error("ResourceConf is null , query resource_conf_num is "+resource_conf_num);		 throw new ServiceException("ResourceConf is null, query ResourceConf confNum is "+resource_conf_num);		}			List<DataSourcePool> sources=dataSourceDao.getBySourceConfNum(conf.getResourceConfNum());				if(CollectionUtils.isEmpty(sources)){			LogUtils.error(" DataSourcePool is null , query resource_conf_num is "+resource_conf_num);			 throw new ServiceException("query DataSourcePool confNum is "+resource_conf_num);	    }				List<CrawlParams> list = new ArrayList<CrawlParams>(); 		for(DataSourcePool source:sources){			if(STATUS_ON==source.getStatus()){				CrawlParams crawl=creatParams(source);				if(null==crawl){					LogUtils.error(" CrawlParams is null , creatParams  source_num is "+source.getSourceNum());				}				crawl.setTaskNum(taskNum);				list.add(crawl);			}		 }		if(CollectionUtils.isEmpty(list)){			LogUtils.error(" CrawlParams  is null , creatParams resource_conf_num is "+resource_conf_num);			 throw new ServiceException("creatParams CrawlParams confNum is "+resource_conf_num);	    }		startCrawl(list);	}	@Override	public void crawlByResourceNum(String resource_num,String taskNum) throws ServiceException {		if(StringUtils.isEmpty(resource_num)){	  throw new NullPointerException("resource_num  is  empty");		}		DataSourcePool source=dataSourceDao.getBySourceNum(resource_num);		if(null==source){		throw new ServiceException(Error.NULL_ERROR) ;			}		List<CrawlParams> list = new ArrayList<CrawlParams>(); 		if(STATUS_ON==source.getStatus()){			CrawlParams crawl=creatParams(source);			if(null==crawl){				LogUtils.error(" CrawlParams is null , creatParams  source_num is "+source.getSourceNum());				throw new ServiceException("creatParams CrawlParams resource_num is "+resource_num);			}			crawl.setTaskNum(taskNum);			list.add(crawl);		}			startCrawl(list);	}		//创建抓取参数	private CrawlParams creatParams(DataSourcePool source) throws ServiceException{		CrawlParams crawl =new CrawlParams();		CrawlConf crawlConf=crawlConfDao.getByCrawlConfNum(source.getCrawlConfNum());		if(null==crawlConf){			LogUtils.error(" crawlConf  is null ");			 throw new ServiceException(Error.NULL_ERROR);		}		List<Metadata> metadatas =metadataDao.getByMetadataModelNum(source.getMetadataModelNum());		ReqParams params=ReqParams.parserParams(source.getReqParams());		UrlRegFilters filters=UrlRegFilters.parserRegx(source.getUrlFilters());		crawl.setCrawlConf(crawlConf);		crawl.setReqParams(params);		crawl.setUrlRegFilters(filters);		crawl.setData(source);		crawl.setMetadatas(metadatas);		if(STATUS_ON==crawlConf.getIpProxy()){			IpProxyPool proxy=ipProxyPoolDao.getByProxyIp("001");		crawl.setIpProxyPool(proxy);		}		return crawl;	}	@Override	public void startCrawl(List<CrawlParams> params) throws ServiceException {				if(CollectionUtils.isEmpty(params)){			LogUtils.error(" CrawlParams  is null ");			 throw new ServiceException(Error.NULL_ERROR);	    }				List<CrawlController> list = new ArrayList<CrawlController>();				  for(CrawlParams crawl:params){				 DataSourcePool sourceData=crawl.getData();//数据源						 ReqParams reqParams=crawl.getReqParams();//请求参数						 CrawlConf crawlConf=crawl.getCrawlConf();//爬虫参数配置						 IpProxyPool proxy=crawl.getIpProxyPool();						String crawlStorageFolder = CrawlInfoConfig.pidpath+"/"+sourceData.getSourceNum();			CrawlConfig config = new CrawlConfig();			config.setCrawlStorageFolder(crawlStorageFolder);			config.setPolitenessDelay(crawlConf.getDelayTime());			config.setMaxDepthOfCrawling(crawlConf.getMaxDepth());			config.setMaxPagesToFetch(crawlConf.getMaxPages());			if(null!=reqParams){			config.setCookies(reqParams.getCookies());			config.setReqParams(reqParams.getReqParams());			}			//@TODO 设置IP 代理 			if(STATUS_ON==crawlConf.getIpProxy() && null!=proxy){				config.setProxyHost(proxy.getProxyIp());				config.setProxyPort(80);				config.setProxyUsername(proxy.getProxyAccount()); 				config.setProxyPassword(proxy.getProxyPwd());			}			config.setResumableCrawling(false);						PageFetcher pageFetcher = new PageFetcher(config);			RobotstxtConfig robotstxtConfig = new RobotstxtConfig();			robotstxtConfig.setEnabled(STATUS_ON==crawlConf.getIsRobots());			RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);			try {				CrawlController controller= new CrawlController(config, pageFetcher, robotstxtServer);				controller.setCustomData(crawl);				controller.addSeed(sourceData.getDataSource());			    controller.startNonBlocking(WebCrawlCore.class, crawlConf.getThreadNum());			    list.add(controller);			    Thread.sleep(2000);				} catch (Exception e) {					LogUtils.errorLog(e.getMessage(), e);					throw new ServiceException(Error.ACTION_ERROR);				}		  }		  		 for(CrawlController controller:list){				//等待所有线程结束				controller.waitUntilFinish();				//获取本地存储数据					List<Object> localData = controller.getCrawlersLocalData();				for(Object obj:localData){					CrawlResult result=(CrawlResult)obj;					CrawlParams crawl=(CrawlParams)controller.getCustomData();					System.out.println("params: "+crawl.getCrawlResult().toString());					LogUtils.debug(result.toString());				}		  }		  			}}